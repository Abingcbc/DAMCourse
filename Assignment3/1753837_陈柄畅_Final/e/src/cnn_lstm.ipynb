{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 3, 4, 32)          320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 2, 64)          18496     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              132096    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 891)               457083    \n",
      "=================================================================\n",
      "Total params: 1,395,451\n",
      "Trainable params: 1,395,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "10047/10047 [==============================] - 3s 332us/step - loss: 5.4055 - acc: 0.0358\n",
      "Epoch 2/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 4.9032 - acc: 0.0631\n",
      "Epoch 3/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 4.6573 - acc: 0.0751\n",
      "Epoch 4/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 4.4542 - acc: 0.0864\n",
      "Epoch 5/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 4.2150 - acc: 0.1105\n",
      "Epoch 6/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 3.9764 - acc: 0.1289\n",
      "Epoch 7/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 3.7992 - acc: 0.1420\n",
      "Epoch 8/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 3.6309 - acc: 0.1544\n",
      "Epoch 9/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 3.5071 - acc: 0.1721\n",
      "Epoch 10/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 3.3601 - acc: 0.1871\n",
      "Epoch 11/200\n",
      "10047/10047 [==============================] - 1s 149us/step - loss: 3.2274 - acc: 0.2024\n",
      "Epoch 12/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 3.0986 - acc: 0.2212\n",
      "Epoch 13/200\n",
      "10047/10047 [==============================] - 2s 155us/step - loss: 3.0028 - acc: 0.2323\n",
      "Epoch 14/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 2.8647 - acc: 0.2506\n",
      "Epoch 15/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 2.7494 - acc: 0.2781\n",
      "Epoch 16/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 2.6112 - acc: 0.2965\n",
      "Epoch 17/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 2.4977 - acc: 0.3232\n",
      "Epoch 18/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 2.3868 - acc: 0.3405\n",
      "Epoch 19/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 2.2977 - acc: 0.3549\n",
      "Epoch 20/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 2.1918 - acc: 0.3806\n",
      "Epoch 21/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 2.1001 - acc: 0.3984\n",
      "Epoch 22/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 1.9848 - acc: 0.4195\n",
      "Epoch 23/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 1.9120 - acc: 0.4426\n",
      "Epoch 24/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 1.8271 - acc: 0.4562\n",
      "Epoch 25/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 1.7532 - acc: 0.4709\n",
      "Epoch 26/200\n",
      "10047/10047 [==============================] - 1s 148us/step - loss: 1.7201 - acc: 0.4816\n",
      "Epoch 27/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 1.6547 - acc: 0.4952\n",
      "Epoch 28/200\n",
      "10047/10047 [==============================] - 1s 148us/step - loss: 1.5624 - acc: 0.5181\n",
      "Epoch 29/200\n",
      "10047/10047 [==============================] - 1s 139us/step - loss: 1.5102 - acc: 0.5250\n",
      "Epoch 30/200\n",
      "10047/10047 [==============================] - 1s 137us/step - loss: 1.4697 - acc: 0.5394\n",
      "Epoch 31/200\n",
      "10047/10047 [==============================] - 1s 147us/step - loss: 1.4240 - acc: 0.5506\n",
      "Epoch 32/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 1.3743 - acc: 0.5621\n",
      "Epoch 33/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 1.3551 - acc: 0.5659\n",
      "Epoch 34/200\n",
      "10047/10047 [==============================] - 1s 145us/step - loss: 1.2940 - acc: 0.5818\n",
      "Epoch 35/200\n",
      "10047/10047 [==============================] - 1s 136us/step - loss: 1.2621 - acc: 0.5919\n",
      "Epoch 36/200\n",
      "10047/10047 [==============================] - 1s 137us/step - loss: 1.2214 - acc: 0.5996\n",
      "Epoch 37/200\n",
      "10047/10047 [==============================] - 1s 145us/step - loss: 1.1966 - acc: 0.6073\n",
      "Epoch 38/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 1.1599 - acc: 0.6206\n",
      "Epoch 39/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 1.1285 - acc: 0.6221\n",
      "Epoch 40/200\n",
      "10047/10047 [==============================] - 1s 149us/step - loss: 1.1165 - acc: 0.6297\n",
      "Epoch 41/200\n",
      "10047/10047 [==============================] - 1s 138us/step - loss: 1.0671 - acc: 0.6458\n",
      "Epoch 42/200\n",
      "10047/10047 [==============================] - 1s 140us/step - loss: 1.0469 - acc: 0.6430\n",
      "Epoch 43/200\n",
      "10047/10047 [==============================] - 1s 141us/step - loss: 1.0164 - acc: 0.6585\n",
      "Epoch 44/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.9912 - acc: 0.6620\n",
      "Epoch 45/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 1.0033 - acc: 0.6597\n",
      "Epoch 46/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.9518 - acc: 0.6772\n",
      "Epoch 47/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.9249 - acc: 0.6846\n",
      "Epoch 48/200\n",
      "10047/10047 [==============================] - 1s 148us/step - loss: 0.9250 - acc: 0.6862\n",
      "Epoch 49/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.9046 - acc: 0.6851\n",
      "Epoch 50/200\n",
      "10047/10047 [==============================] - 1s 149us/step - loss: 0.8945 - acc: 0.6934\n",
      "Epoch 51/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.8803 - acc: 0.6970\n",
      "Epoch 52/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.8390 - acc: 0.7076\n",
      "Epoch 53/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.8290 - acc: 0.7116\n",
      "Epoch 54/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.8234 - acc: 0.7133\n",
      "Epoch 55/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.7859 - acc: 0.7315\n",
      "Epoch 56/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.7759 - acc: 0.7286\n",
      "Epoch 57/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.7722 - acc: 0.7267\n",
      "Epoch 58/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.7743 - acc: 0.7265\n",
      "Epoch 59/200\n",
      "10047/10047 [==============================] - 2s 156us/step - loss: 0.7371 - acc: 0.7401\n",
      "Epoch 60/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.7484 - acc: 0.7335\n",
      "Epoch 61/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.7341 - acc: 0.7464\n",
      "Epoch 62/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.7129 - acc: 0.7462\n",
      "Epoch 63/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.7152 - acc: 0.7492\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.7044 - acc: 0.7504\n",
      "Epoch 65/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.6766 - acc: 0.7612\n",
      "Epoch 66/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.6950 - acc: 0.7586\n",
      "Epoch 67/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.6795 - acc: 0.7580\n",
      "Epoch 68/200\n",
      "10047/10047 [==============================] - 1s 141us/step - loss: 0.6647 - acc: 0.7640\n",
      "Epoch 69/200\n",
      "10047/10047 [==============================] - 1s 135us/step - loss: 0.6622 - acc: 0.7645\n",
      "Epoch 70/200\n",
      "10047/10047 [==============================] - 1s 137us/step - loss: 0.6555 - acc: 0.7643\n",
      "Epoch 71/200\n",
      "10047/10047 [==============================] - 1s 137us/step - loss: 0.6343 - acc: 0.7738\n",
      "Epoch 72/200\n",
      "10047/10047 [==============================] - 1s 135us/step - loss: 0.6232 - acc: 0.7749\n",
      "Epoch 73/200\n",
      "10047/10047 [==============================] - 1s 143us/step - loss: 0.6266 - acc: 0.7757\n",
      "Epoch 74/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.6149 - acc: 0.7779\n",
      "Epoch 75/200\n",
      "10047/10047 [==============================] - 1s 141us/step - loss: 0.6061 - acc: 0.7805\n",
      "Epoch 76/200\n",
      "10047/10047 [==============================] - 1s 135us/step - loss: 0.6081 - acc: 0.7822\n",
      "Epoch 77/200\n",
      "10047/10047 [==============================] - 1s 147us/step - loss: 0.6142 - acc: 0.7796\n",
      "Epoch 78/200\n",
      "10047/10047 [==============================] - 1s 146us/step - loss: 0.5971 - acc: 0.7866\n",
      "Epoch 79/200\n",
      "10047/10047 [==============================] - 1s 148us/step - loss: 0.5983 - acc: 0.7864\n",
      "Epoch 80/200\n",
      "10047/10047 [==============================] - 1s 147us/step - loss: 0.5960 - acc: 0.7937\n",
      "Epoch 81/200\n",
      "10047/10047 [==============================] - 1s 140us/step - loss: 0.5771 - acc: 0.7924\n",
      "Epoch 82/200\n",
      "10047/10047 [==============================] - 1s 138us/step - loss: 0.5805 - acc: 0.7875\n",
      "Epoch 83/200\n",
      "10047/10047 [==============================] - 1s 149us/step - loss: 0.5900 - acc: 0.7919\n",
      "Epoch 84/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.5617 - acc: 0.7963\n",
      "Epoch 85/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.5527 - acc: 0.7996\n",
      "Epoch 86/200\n",
      "10047/10047 [==============================] - 1s 149us/step - loss: 0.5374 - acc: 0.8043\n",
      "Epoch 87/200\n",
      "10047/10047 [==============================] - 1s 149us/step - loss: 0.5516 - acc: 0.8014\n",
      "Epoch 88/200\n",
      "10047/10047 [==============================] - 1s 141us/step - loss: 0.5462 - acc: 0.8012\n",
      "Epoch 89/200\n",
      "10047/10047 [==============================] - 1s 138us/step - loss: 0.5279 - acc: 0.8071\n",
      "Epoch 90/200\n",
      "10047/10047 [==============================] - 1s 145us/step - loss: 0.5425 - acc: 0.8005\n",
      "Epoch 91/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.5412 - acc: 0.8041\n",
      "Epoch 92/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.5280 - acc: 0.8086\n",
      "Epoch 93/200\n",
      "10047/10047 [==============================] - 1s 149us/step - loss: 0.5014 - acc: 0.8200\n",
      "Epoch 94/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.5154 - acc: 0.8139\n",
      "Epoch 95/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.5107 - acc: 0.8116\n",
      "Epoch 96/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.4945 - acc: 0.8229\n",
      "Epoch 97/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.5000 - acc: 0.8174\n",
      "Epoch 98/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.5159 - acc: 0.8080\n",
      "Epoch 99/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.5013 - acc: 0.8177\n",
      "Epoch 100/200\n",
      "10047/10047 [==============================] - 2s 155us/step - loss: 0.4775 - acc: 0.8225\n",
      "Epoch 101/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.4983 - acc: 0.8183\n",
      "Epoch 102/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.5061 - acc: 0.8167\n",
      "Epoch 103/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.4811 - acc: 0.8263\n",
      "Epoch 104/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.4823 - acc: 0.8240\n",
      "Epoch 105/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.4770 - acc: 0.8247\n",
      "Epoch 106/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.4589 - acc: 0.8330\n",
      "Epoch 107/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.4744 - acc: 0.8255\n",
      "Epoch 108/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.4797 - acc: 0.8271\n",
      "Epoch 109/200\n",
      "10047/10047 [==============================] - 1s 146us/step - loss: 0.4529 - acc: 0.8344\n",
      "Epoch 110/200\n",
      "10047/10047 [==============================] - 1s 137us/step - loss: 0.4632 - acc: 0.8324\n",
      "Epoch 111/200\n",
      "10047/10047 [==============================] - 1s 139us/step - loss: 0.4701 - acc: 0.8312\n",
      "Epoch 112/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.4704 - acc: 0.8294\n",
      "Epoch 113/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.4558 - acc: 0.8312\n",
      "Epoch 114/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.4489 - acc: 0.8335\n",
      "Epoch 115/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.4394 - acc: 0.8436\n",
      "Epoch 116/200\n",
      "10047/10047 [==============================] - 1s 138us/step - loss: 0.4715 - acc: 0.8268\n",
      "Epoch 117/200\n",
      "10047/10047 [==============================] - 1s 132us/step - loss: 0.4406 - acc: 0.8371\n",
      "Epoch 118/200\n",
      "10047/10047 [==============================] - 1s 140us/step - loss: 0.4316 - acc: 0.8422\n",
      "Epoch 119/200\n",
      "10047/10047 [==============================] - 1s 136us/step - loss: 0.4335 - acc: 0.8379\n",
      "Epoch 120/200\n",
      "10047/10047 [==============================] - 1s 138us/step - loss: 0.4358 - acc: 0.8402\n",
      "Epoch 121/200\n",
      "10047/10047 [==============================] - 1s 144us/step - loss: 0.4454 - acc: 0.8392\n",
      "Epoch 122/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.4174 - acc: 0.8481\n",
      "Epoch 123/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.4267 - acc: 0.8440\n",
      "Epoch 124/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.4266 - acc: 0.8384\n",
      "Epoch 125/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.4157 - acc: 0.8492\n",
      "Epoch 126/200\n",
      "10047/10047 [==============================] - 1s 149us/step - loss: 0.4347 - acc: 0.8412\n",
      "Epoch 127/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.4238 - acc: 0.8468\n",
      "Epoch 128/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.4163 - acc: 0.8483\n",
      "Epoch 129/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.4116 - acc: 0.8455\n",
      "Epoch 130/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.4105 - acc: 0.8511\n",
      "Epoch 131/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.4253 - acc: 0.8435\n",
      "Epoch 132/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.3921 - acc: 0.8559\n",
      "Epoch 133/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.4011 - acc: 0.8521\n",
      "Epoch 134/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.4049 - acc: 0.8507\n",
      "Epoch 135/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.4115 - acc: 0.8481\n",
      "Epoch 136/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.4007 - acc: 0.8556\n",
      "Epoch 137/200\n",
      "10047/10047 [==============================] - 2s 156us/step - loss: 0.4145 - acc: 0.8505\n",
      "Epoch 138/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.4067 - acc: 0.8524\n",
      "Epoch 139/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.3985 - acc: 0.8521\n",
      "Epoch 140/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3846 - acc: 0.8558\n",
      "Epoch 141/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3899 - acc: 0.8571\n",
      "Epoch 142/200\n",
      "10047/10047 [==============================] - 1s 147us/step - loss: 0.3882 - acc: 0.8593\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.3821 - acc: 0.8555\n",
      "Epoch 144/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.3999 - acc: 0.8541\n",
      "Epoch 145/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.3907 - acc: 0.8551\n",
      "Epoch 146/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3818 - acc: 0.8600\n",
      "Epoch 147/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.3951 - acc: 0.8555\n",
      "Epoch 148/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3730 - acc: 0.8634\n",
      "Epoch 149/200\n",
      "10047/10047 [==============================] - 1s 141us/step - loss: 0.3850 - acc: 0.8586\n",
      "Epoch 150/200\n",
      "10047/10047 [==============================] - 1s 137us/step - loss: 0.3747 - acc: 0.8591\n",
      "Epoch 151/200\n",
      "10047/10047 [==============================] - 1s 141us/step - loss: 0.3749 - acc: 0.8616\n",
      "Epoch 152/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.3847 - acc: 0.8602\n",
      "Epoch 153/200\n",
      "10047/10047 [==============================] - 2s 155us/step - loss: 0.3542 - acc: 0.8673\n",
      "Epoch 154/200\n",
      "10047/10047 [==============================] - 1s 146us/step - loss: 0.3684 - acc: 0.8596\n",
      "Epoch 155/200\n",
      "10047/10047 [==============================] - 1s 141us/step - loss: 0.3854 - acc: 0.8566\n",
      "Epoch 156/200\n",
      "10047/10047 [==============================] - 1s 138us/step - loss: 0.3656 - acc: 0.8640\n",
      "Epoch 157/200\n",
      "10047/10047 [==============================] - 1s 144us/step - loss: 0.3790 - acc: 0.8601\n",
      "Epoch 158/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.3705 - acc: 0.8600\n",
      "Epoch 159/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.3578 - acc: 0.8682\n",
      "Epoch 160/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.3441 - acc: 0.8697\n",
      "Epoch 161/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3519 - acc: 0.8675\n",
      "Epoch 162/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.3638 - acc: 0.8653\n",
      "Epoch 163/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3616 - acc: 0.8681\n",
      "Epoch 164/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.3413 - acc: 0.8750\n",
      "Epoch 165/200\n",
      "10047/10047 [==============================] - 1s 148us/step - loss: 0.3515 - acc: 0.8710\n",
      "Epoch 166/200\n",
      "10047/10047 [==============================] - 1s 139us/step - loss: 0.3623 - acc: 0.8687\n",
      "Epoch 167/200\n",
      "10047/10047 [==============================] - 1s 137us/step - loss: 0.3546 - acc: 0.8664\n",
      "Epoch 168/200\n",
      "10047/10047 [==============================] - 1s 148us/step - loss: 0.3632 - acc: 0.8683\n",
      "Epoch 169/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.3580 - acc: 0.8673\n",
      "Epoch 170/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.3445 - acc: 0.8709\n",
      "Epoch 171/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.3596 - acc: 0.8642\n",
      "Epoch 172/200\n",
      "10047/10047 [==============================] - 1s 149us/step - loss: 0.3490 - acc: 0.8685\n",
      "Epoch 173/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3349 - acc: 0.8722\n",
      "Epoch 174/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.3324 - acc: 0.8755\n",
      "Epoch 175/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3504 - acc: 0.8727\n",
      "Epoch 176/200\n",
      "10047/10047 [==============================] - 2s 156us/step - loss: 0.3383 - acc: 0.8724\n",
      "Epoch 177/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3412 - acc: 0.8765\n",
      "Epoch 178/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.3368 - acc: 0.8760\n",
      "Epoch 179/200\n",
      "10047/10047 [==============================] - 1s 148us/step - loss: 0.3432 - acc: 0.8740\n",
      "Epoch 180/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.3274 - acc: 0.8777\n",
      "Epoch 181/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.3415 - acc: 0.8757\n",
      "Epoch 182/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.3363 - acc: 0.8749\n",
      "Epoch 183/200\n",
      "10047/10047 [==============================] - 2s 155us/step - loss: 0.3561 - acc: 0.8724\n",
      "Epoch 184/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3323 - acc: 0.8796\n",
      "Epoch 185/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.3268 - acc: 0.8778\n",
      "Epoch 186/200\n",
      "10047/10047 [==============================] - 2s 152us/step - loss: 0.3245 - acc: 0.8803\n",
      "Epoch 187/200\n",
      "10047/10047 [==============================] - 2s 150us/step - loss: 0.3384 - acc: 0.8770\n",
      "Epoch 188/200\n",
      "10047/10047 [==============================] - 2s 151us/step - loss: 0.3381 - acc: 0.8757\n",
      "Epoch 189/200\n",
      "10047/10047 [==============================] - 1s 144us/step - loss: 0.3156 - acc: 0.8826\n",
      "Epoch 190/200\n",
      "10047/10047 [==============================] - 1s 138us/step - loss: 0.3258 - acc: 0.8811\n",
      "Epoch 191/200\n",
      "10047/10047 [==============================] - 1s 138us/step - loss: 0.3409 - acc: 0.8772\n",
      "Epoch 192/200\n",
      "10047/10047 [==============================] - 1s 148us/step - loss: 0.3374 - acc: 0.8781\n",
      "Epoch 193/200\n",
      "10047/10047 [==============================] - 2s 154us/step - loss: 0.3311 - acc: 0.8800\n",
      "Epoch 194/200\n",
      "10047/10047 [==============================] - 1s 147us/step - loss: 0.3276 - acc: 0.8778\n",
      "Epoch 195/200\n",
      "10047/10047 [==============================] - 1s 136us/step - loss: 0.3095 - acc: 0.8844\n",
      "Epoch 196/200\n",
      "10047/10047 [==============================] - 1s 138us/step - loss: 0.3094 - acc: 0.8887\n",
      "Epoch 197/200\n",
      "10047/10047 [==============================] - 1s 143us/step - loss: 0.3133 - acc: 0.8849\n",
      "Epoch 198/200\n",
      "10047/10047 [==============================] - 2s 153us/step - loss: 0.2937 - acc: 0.8920\n",
      "Epoch 199/200\n",
      "10047/10047 [==============================] - 1s 146us/step - loss: 0.3135 - acc: 0.8813\n",
      "Epoch 200/200\n",
      "10047/10047 [==============================] - 1s 139us/step - loss: 0.3128 - acc: 0.8839\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras import optimizers\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Masking\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import TimeDistributed\n",
    "import utm\n",
    "\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch': [], 'epoch': []}\n",
    "        self.accuracy = {'batch': [], 'epoch': []}\n",
    "        self.val_loss = {'batch': [], 'epoch': []}\n",
    "        self.val_acc = {'batch': [], 'epoch': []}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "\n",
    "data = pd.read_csv('trainfix.csv')\n",
    "traj=data['TrajID'].values\n",
    "ao=data[['Latitude','Longitude']].values.T\n",
    "\n",
    "dataset = [\n",
    "    np.array([[mr[4:9],\n",
    "               mr[9:14],\n",
    "               mr[14:19],\n",
    "               mr[19:24],\n",
    "               mr[24:29],\n",
    "               mr[29:34]]]).T for mr in data.values\n",
    "]\n",
    "\n",
    "dataset = np.array(dataset)\n",
    "label = pd.read_csv('label1.csv')['label'].values\n",
    "\n",
    "labelset=to_categorical(label,891)\n",
    "\n",
    "def build_model_cnn():\n",
    "    m = Sequential()\n",
    "    m.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(5, 6, 1)))\n",
    "    m.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "    m.add(Flatten())\n",
    "    m.add(Dense(1024, activation='relu'))\n",
    "    m.add(Dropout(0.2))\n",
    "    m.add(Dense(512, activation='relu'))\n",
    "    m.add(Dropout(0.2))\n",
    "    m.add(Dense(512, activation='relu'))\n",
    "    m.add(Dropout(0.2))\n",
    "    m.add(Dense(891, activation='softmax'))\n",
    "    r = optimizers.Adam(lr=0.001)\n",
    "    m.compile(optimizer=r, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "def build_model_lstm():\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(32,return_sequences=True,input_shape=(maxleng,891)))\n",
    "    model.add(LSTM(64,return_sequences=True))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(TimeDistributed(Dense(891)))\n",
    "    model.add(Activation('softmax'))\n",
    "    r = RMSprop(lr=0.001)\n",
    "    model.compile(optimizer=r,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "x_train,x_test, y_train, y_test = train_test_split(dataset, labelset, test_size=0.2, random_state=33)\n",
    "model = build_model_cnn()\n",
    "model.summary()\n",
    "model.fit(dataset, labelset, epochs=200, batch_size=64)\n",
    "# model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=200, batch_size=64)\n",
    "predict=model.predict(dataset)\n",
    "predict=pd.DataFrame(predict)\n",
    "predict['TrajID']=traj\n",
    "predict['Latitude']=ao[0]\n",
    "predict['Longitude']=ao[1]\n",
    "maxleng=6\n",
    "\n",
    "tra=predict[\"TrajID\"].unique()\n",
    "datasetL=[]\n",
    "labelsetL=[]\n",
    "for i in tra:\n",
    "       x=predict[predict[\"TrajID\"]==i]\n",
    "       value=[mr[0:891]for mr in x.values]\n",
    "       datasetL.append(value)\n",
    "       labelsetL.append(x[['Latitude','Longitude']].values)\n",
    "datasetfix=[]\n",
    "labelsetfix=[]\n",
    "for i in range(len(datasetL)):\n",
    "       count=len(datasetL[i])-maxleng+1\n",
    "       for j in range(count):\n",
    "              datasetfix.append(datasetL[i][j:j+maxleng])\n",
    "              labelsetfix.append(labelsetL[i][j:j+maxleng])\n",
    "datasetfix=np.array(datasetfix)\n",
    "labelsetfix=np.array(labelsetfix)\n",
    "X_train, X_test, y_train, y_test = train_test_split(datasetfix, labelsetfix, test_size=0.2, random_state=33)\n",
    "\n",
    "grid=pd.read_csv('grid1.csv')[['x','y']].values\n",
    "Y_train=[]\n",
    "for seq in y_train:\n",
    "    poses=[]\n",
    "    for mr in seq:\n",
    "        u1, u2, _, _ = utm.from_latlon(mr[0], mr[1])\n",
    "        test = np.array([u1, u2])\n",
    "        poses.append(np.argmin(np.sum(np.square(test - grid), axis=1)))\n",
    "    Y_train.append(poses)\n",
    "\n",
    "Y_train=to_categorical(np.array(Y_train),891)\n",
    "Y_test=[]\n",
    "y_test_utm=[]\n",
    "for seq in y_test:\n",
    "    poses=[]\n",
    "    utms=[]\n",
    "    for mr in seq:\n",
    "        u1, u2, _, _ = utm.from_latlon(mr[0], mr[1])\n",
    "        test = np.array([u1, u2])\n",
    "        utms.append([u1, u2])\n",
    "        poses.append(np.argmin(np.sum(np.square(test - grid), axis=1)))\n",
    "    y_test_utm.append(utms)\n",
    "    Y_test.append(poses)\n",
    "\n",
    "Y_test=to_categorical(np.array(Y_test),891)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 6, 32)             118272    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 6, 64)             24832     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6, 1024)           66560     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 6, 891)            913275    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6, 891)            0         \n",
      "=================================================================\n",
      "Total params: 1,122,939\n",
      "Trainable params: 1,122,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "9687/9687 [==============================] - 9s 977us/step - loss: 5.3867 - acc: 0.0258\n",
      "Epoch 2/100\n",
      "9687/9687 [==============================] - 9s 964us/step - loss: 5.2651 - acc: 0.0269\n",
      "Epoch 3/100\n",
      "9687/9687 [==============================] - 9s 950us/step - loss: 5.2516 - acc: 0.0287\n",
      "Epoch 4/100\n",
      "9687/9687 [==============================] - 9s 884us/step - loss: 5.2368 - acc: 0.0289\n",
      "Epoch 5/100\n",
      "9687/9687 [==============================] - 9s 887us/step - loss: 5.2157 - acc: 0.0300\n",
      "Epoch 6/100\n",
      "9687/9687 [==============================] - 9s 896us/step - loss: 5.1970 - acc: 0.0308\n",
      "Epoch 7/100\n",
      "9687/9687 [==============================] - 9s 887us/step - loss: 5.1807 - acc: 0.0330\n",
      "Epoch 8/100\n",
      "9687/9687 [==============================] - 9s 940us/step - loss: 5.1693 - acc: 0.0333\n",
      "Epoch 9/100\n",
      "9687/9687 [==============================] - 9s 934us/step - loss: 5.1548 - acc: 0.0348\n",
      "Epoch 10/100\n",
      "9687/9687 [==============================] - 9s 948us/step - loss: 5.1463 - acc: 0.0354\n",
      "Epoch 11/100\n",
      "9687/9687 [==============================] - 9s 913us/step - loss: 5.1376 - acc: 0.0365\n",
      "Epoch 12/100\n",
      "9687/9687 [==============================] - 9s 920us/step - loss: 5.1295 - acc: 0.0370\n",
      "Epoch 13/100\n",
      "9687/9687 [==============================] - 9s 954us/step - loss: 5.1201 - acc: 0.0380\n",
      "Epoch 14/100\n",
      "9687/9687 [==============================] - 9s 912us/step - loss: 5.1147 - acc: 0.0391\n",
      "Epoch 15/100\n",
      "9687/9687 [==============================] - 9s 958us/step - loss: 5.1047 - acc: 0.0386\n",
      "Epoch 16/100\n",
      "9687/9687 [==============================] - 9s 942us/step - loss: 5.0968 - acc: 0.0386\n",
      "Epoch 17/100\n",
      "9687/9687 [==============================] - 9s 925us/step - loss: 5.0890 - acc: 0.0397\n",
      "Epoch 18/100\n",
      "9687/9687 [==============================] - 9s 907us/step - loss: 5.0812 - acc: 0.0416\n",
      "Epoch 19/100\n",
      "9687/9687 [==============================] - 9s 914us/step - loss: 5.0737 - acc: 0.0396\n",
      "Epoch 20/100\n",
      "9687/9687 [==============================] - 9s 908us/step - loss: 5.0700 - acc: 0.0409\n",
      "Epoch 21/100\n",
      "9687/9687 [==============================] - 9s 942us/step - loss: 5.0577 - acc: 0.0420\n",
      "Epoch 22/100\n",
      "9687/9687 [==============================] - 9s 949us/step - loss: 5.0522 - acc: 0.0422\n",
      "Epoch 23/100\n",
      "9687/9687 [==============================] - 9s 952us/step - loss: 5.0435 - acc: 0.0430\n",
      "Epoch 24/100\n",
      "9687/9687 [==============================] - 9s 905us/step - loss: 5.0350 - acc: 0.0445\n",
      "Epoch 25/100\n",
      "9687/9687 [==============================] - 9s 911us/step - loss: 5.0283 - acc: 0.0440\n",
      "Epoch 26/100\n",
      "9687/9687 [==============================] - 9s 905us/step - loss: 5.0194 - acc: 0.0454\n",
      "Epoch 27/100\n",
      "9687/9687 [==============================] - 9s 957us/step - loss: 5.0115 - acc: 0.0459\n",
      "Epoch 28/100\n",
      "9687/9687 [==============================] - 9s 942us/step - loss: 5.0004 - acc: 0.0472\n",
      "Epoch 29/100\n",
      "9687/9687 [==============================] - 9s 950us/step - loss: 4.9939 - acc: 0.0474\n",
      "Epoch 30/100\n",
      "9687/9687 [==============================] - 9s 945us/step - loss: 4.9884 - acc: 0.0469\n",
      "Epoch 31/100\n",
      "9687/9687 [==============================] - 8s 877us/step - loss: 4.9736 - acc: 0.0478\n",
      "Epoch 32/100\n",
      "9687/9687 [==============================] - 9s 908us/step - loss: 4.9693 - acc: 0.0472\n",
      "Epoch 33/100\n",
      "9687/9687 [==============================] - 9s 883us/step - loss: 4.9546 - acc: 0.0502\n",
      "Epoch 34/100\n",
      "9687/9687 [==============================] - 9s 919us/step - loss: 4.9448 - acc: 0.0486\n",
      "Epoch 35/100\n",
      "9687/9687 [==============================] - 9s 951us/step - loss: 4.9331 - acc: 0.0491\n",
      "Epoch 36/100\n",
      "9687/9687 [==============================] - 9s 953us/step - loss: 4.9224 - acc: 0.0509\n",
      "Epoch 37/100\n",
      "9687/9687 [==============================] - 9s 933us/step - loss: 4.9092 - acc: 0.0508\n",
      "Epoch 38/100\n",
      "9687/9687 [==============================] - 9s 911us/step - loss: 4.9020 - acc: 0.0519\n",
      "Epoch 39/100\n",
      "9687/9687 [==============================] - 9s 929us/step - loss: 4.8924 - acc: 0.0519\n",
      "Epoch 40/100\n",
      "9687/9687 [==============================] - 9s 902us/step - loss: 4.8802 - acc: 0.0517\n",
      "Epoch 41/100\n",
      "9687/9687 [==============================] - 9s 936us/step - loss: 4.8722 - acc: 0.0525\n",
      "Epoch 42/100\n",
      "9687/9687 [==============================] - 9s 945us/step - loss: 4.8606 - acc: 0.0527\n",
      "Epoch 43/100\n",
      "9687/9687 [==============================] - 9s 952us/step - loss: 4.8506 - acc: 0.0527\n",
      "Epoch 44/100\n",
      "9687/9687 [==============================] - 9s 907us/step - loss: 4.8400 - acc: 0.0558\n",
      "Epoch 45/100\n",
      "9687/9687 [==============================] - 9s 904us/step - loss: 4.8352 - acc: 0.0546\n",
      "Epoch 46/100\n",
      "9687/9687 [==============================] - 9s 904us/step - loss: 4.8223 - acc: 0.0555\n",
      "Epoch 47/100\n",
      "9687/9687 [==============================] - 9s 939us/step - loss: 4.8169 - acc: 0.0562\n",
      "Epoch 48/100\n",
      "9687/9687 [==============================] - 9s 945us/step - loss: 4.8125 - acc: 0.0574\n",
      "Epoch 49/100\n",
      "9687/9687 [==============================] - 9s 945us/step - loss: 4.7967 - acc: 0.0565\n",
      "Epoch 50/100\n",
      "9687/9687 [==============================] - 9s 952us/step - loss: 4.7973 - acc: 0.0582\n",
      "Epoch 51/100\n",
      "9687/9687 [==============================] - 9s 905us/step - loss: 4.7858 - acc: 0.0607\n",
      "Epoch 52/100\n",
      "9687/9687 [==============================] - 9s 910us/step - loss: 4.7781 - acc: 0.0613\n",
      "Epoch 53/100\n",
      "9687/9687 [==============================] - 9s 942us/step - loss: 4.7738 - acc: 0.0617\n",
      "Epoch 54/100\n",
      "9687/9687 [==============================] - 9s 920us/step - loss: 4.7652 - acc: 0.0615\n",
      "Epoch 55/100\n",
      "9687/9687 [==============================] - 9s 952us/step - loss: 4.7623 - acc: 0.0618\n",
      "Epoch 56/100\n",
      "9687/9687 [==============================] - 9s 949us/step - loss: 4.7496 - acc: 0.0626\n",
      "Epoch 57/100\n",
      "9687/9687 [==============================] - 9s 924us/step - loss: 4.7469 - acc: 0.0632\n",
      "Epoch 58/100\n",
      "9687/9687 [==============================] - 8s 871us/step - loss: 4.7371 - acc: 0.0644\n",
      "Epoch 59/100\n",
      "9687/9687 [==============================] - 9s 913us/step - loss: 4.7325 - acc: 0.0652\n",
      "Epoch 60/100\n",
      "9687/9687 [==============================] - 9s 924us/step - loss: 4.7215 - acc: 0.0666\n",
      "Epoch 61/100\n",
      "9687/9687 [==============================] - 9s 907us/step - loss: 4.7182 - acc: 0.0664\n",
      "Epoch 62/100\n",
      "9687/9687 [==============================] - 9s 940us/step - loss: 4.7106 - acc: 0.0673\n",
      "Epoch 63/100\n",
      "9687/9687 [==============================] - 9s 948us/step - loss: 4.7078 - acc: 0.0672\n",
      "Epoch 64/100\n",
      "9687/9687 [==============================] - 9s 917us/step - loss: 4.7026 - acc: 0.0684\n",
      "Epoch 65/100\n",
      "9687/9687 [==============================] - 9s 915us/step - loss: 4.6951 - acc: 0.0676\n",
      "Epoch 66/100\n",
      "9687/9687 [==============================] - 9s 909us/step - loss: 4.6916 - acc: 0.0688\n",
      "Epoch 67/100\n",
      "9687/9687 [==============================] - 9s 945us/step - loss: 4.6860 - acc: 0.0712\n",
      "Epoch 68/100\n",
      "9687/9687 [==============================] - 9s 955us/step - loss: 4.6806 - acc: 0.0702\n",
      "Epoch 69/100\n",
      "9687/9687 [==============================] - 9s 955us/step - loss: 4.6731 - acc: 0.0706\n",
      "Epoch 70/100\n",
      "9687/9687 [==============================] - 9s 952us/step - loss: 4.6648 - acc: 0.0728\n",
      "Epoch 71/100\n",
      "9687/9687 [==============================] - 9s 908us/step - loss: 4.6622 - acc: 0.0728\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9687/9687 [==============================] - 9s 901us/step - loss: 4.6567 - acc: 0.0722\n",
      "Epoch 73/100\n",
      "9687/9687 [==============================] - 9s 932us/step - loss: 4.6479 - acc: 0.0735\n",
      "Epoch 74/100\n",
      "9687/9687 [==============================] - 9s 930us/step - loss: 4.6467 - acc: 0.0745\n",
      "Epoch 75/100\n",
      "9687/9687 [==============================] - 9s 935us/step - loss: 4.6405 - acc: 0.0748\n",
      "Epoch 76/100\n",
      "9687/9687 [==============================] - 9s 944us/step - loss: 4.6387 - acc: 0.0772\n",
      "Epoch 77/100\n",
      "9687/9687 [==============================] - 9s 949us/step - loss: 4.6303 - acc: 0.0770\n",
      "Epoch 78/100\n",
      "9687/9687 [==============================] - 9s 905us/step - loss: 4.6270 - acc: 0.0771\n",
      "Epoch 79/100\n",
      "9687/9687 [==============================] - 9s 906us/step - loss: 4.6206 - acc: 0.0780\n",
      "Epoch 80/100\n",
      "9687/9687 [==============================] - 9s 930us/step - loss: 4.6170 - acc: 0.0799\n",
      "Epoch 81/100\n",
      "9687/9687 [==============================] - 9s 946us/step - loss: 4.6147 - acc: 0.0781\n",
      "Epoch 82/100\n",
      "9687/9687 [==============================] - 9s 947us/step - loss: 4.6060 - acc: 0.0796\n",
      "Epoch 83/100\n",
      "9687/9687 [==============================] - 9s 957us/step - loss: 4.5998 - acc: 0.0804\n",
      "Epoch 84/100\n",
      "9687/9687 [==============================] - 9s 923us/step - loss: 4.5965 - acc: 0.0800\n",
      "Epoch 85/100\n",
      "9687/9687 [==============================] - 9s 881us/step - loss: 4.5883 - acc: 0.0814\n",
      "Epoch 86/100\n",
      "9687/9687 [==============================] - 9s 909us/step - loss: 4.5856 - acc: 0.0830\n",
      "Epoch 87/100\n",
      "9687/9687 [==============================] - 9s 883us/step - loss: 4.5839 - acc: 0.0837\n",
      "Epoch 88/100\n",
      "9687/9687 [==============================] - 9s 939us/step - loss: 4.5739 - acc: 0.0840\n",
      "Epoch 89/100\n",
      "9687/9687 [==============================] - 9s 952us/step - loss: 4.5728 - acc: 0.0844\n",
      "Epoch 90/100\n",
      "9687/9687 [==============================] - 9s 953us/step - loss: 4.5649 - acc: 0.0869\n",
      "Epoch 91/100\n",
      "9687/9687 [==============================] - 9s 911us/step - loss: 4.5637 - acc: 0.0853\n",
      "Epoch 92/100\n",
      "9687/9687 [==============================] - 9s 914us/step - loss: 4.5607 - acc: 0.0854\n",
      "Epoch 93/100\n",
      "9687/9687 [==============================] - 9s 913us/step - loss: 4.5548 - acc: 0.0855\n",
      "Epoch 94/100\n",
      "9687/9687 [==============================] - 9s 938us/step - loss: 4.5525 - acc: 0.0870\n",
      "Epoch 95/100\n",
      "9687/9687 [==============================] - 9s 943us/step - loss: 4.5459 - acc: 0.0876\n",
      "Epoch 96/100\n",
      "9687/9687 [==============================] - 9s 959us/step - loss: 4.5418 - acc: 0.0873\n",
      "Epoch 97/100\n",
      "9687/9687 [==============================] - 9s 952us/step - loss: 4.5427 - acc: 0.0872\n",
      "Epoch 98/100\n",
      "9687/9687 [==============================] - 9s 906us/step - loss: 4.5338 - acc: 0.0904\n",
      "Epoch 99/100\n",
      "9687/9687 [==============================] - 8s 877us/step - loss: 4.5339 - acc: 0.0886\n",
      "Epoch 100/100\n",
      "9687/9687 [==============================] - 9s 939us/step - loss: 4.5251 - acc: 0.0886\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = LossHistory()\n",
    "#lstm-----\n",
    "modellstm=build_model_lstm()\n",
    "modellstm.summary()\n",
    "y = np.concatenate([Y_train, Y_test])\n",
    "modellstm.fit(datasetfix, y,epochs=100,batch_size=32, callbacks=[history])\n",
    "# modellstm.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=100,batch_size=32, callbacks=[history])\n",
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def Dist_Error(pred,real):\n",
    "    errors=np.abs(pred-real)\n",
    "    result=[math.sqrt(math.pow(i[0],2)+math.pow(i[1],2))for i in errors]\n",
    "    return np.array(result)\n",
    "testRe=modellstm.predict_classes(X_test)\n",
    "zone_number = 51\n",
    "zone_letter = 'R'\n",
    "testRe=testRe.reshape(-1)\n",
    "y_test_utm=np.array(y_test_utm).reshape(-1,2)\n",
    "\n",
    "testFi=[[grid[i][0], grid[i][1]] for i in testRe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors=Dist_Error(np.array(testFi),np.array(y_test_utm))\n",
    "errors=np.array(errors)\n",
    "print(np.median(errors))\n",
    "print(np.mean(errors))\n",
    "print(np.sort(errors)[int(len(errors)*0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,np.max(errors))\n",
    "error_y = [len(errors[errors < i])/len(errors) for i in x]\n",
    "plt.plot(x, error_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatest = pd.read_csv('testfix.csv')\n",
    "traj=datatest['TrajID'].values\n",
    "\n",
    "dataset = [\n",
    "    np.array([[mr[4:9],\n",
    "               mr[9:14],\n",
    "               mr[14:19],\n",
    "               mr[19:24],\n",
    "               mr[24:29],\n",
    "               mr[29:34]]]).T for mr in datatest.values\n",
    "]\n",
    "\n",
    "dataset = np.array(dataset)\n",
    "predict=model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70,)\n"
     ]
    }
   ],
   "source": [
    "predict=pd.DataFrame(predict)\n",
    "predict['TrajID']=traj\n",
    "tra=predict[\"TrajID\"].unique()\n",
    "datasetL=[]\n",
    "labelsetL=[]\n",
    "for i in tra:\n",
    "       x=predict[predict[\"TrajID\"]==i]\n",
    "       value=[mr[0:891]for mr in x.values]\n",
    "       datasetL.append(value)\n",
    "print(np.shape(datasetL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2120, 6, 891)\n"
     ]
    }
   ],
   "source": [
    "datasetfix=[]\n",
    "traj_id=[]\n",
    "for i in range(len(datasetL)):\n",
    "       count=len(datasetL[i])-maxleng+1\n",
    "       for j in range(count):\n",
    "            datasetfix.append(datasetL[i][j:j+maxleng])\n",
    "            traj_id.append(i)\n",
    "datasetfix=np.array(datasetfix)\n",
    "print(datasetfix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modellstm.predict_classes(datasetfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2120, 6)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def traj_merge(traj_id,trajs):\n",
    "    maxleng=len(trajs[0])\n",
    "    last=-1\n",
    "    result=[]\n",
    "    for i in range(len(traj_id)):\n",
    "        if traj_id[i]!=last:\n",
    "            result+=trajs[i]\n",
    "            last=traj_id[i]\n",
    "        else:\n",
    "            result.append(trajs[i][maxleng-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2470,)\n"
     ]
    }
   ],
   "source": [
    "y_p = traj_merge(traj_id,y_pred.tolist())\n",
    "print(np.shape(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import utm\n",
    "zone_number = 51\n",
    "zone_letter = 'R'\n",
    "y_final = [[utm.to_latlon(grid[i][0], grid[i][1], zone_letter=zone_letter, zone_number=zone_number)[1],\n",
    "           utm.to_latlon(grid[i][0], grid[i][1], zone_letter=zone_letter, zone_number=zone_number)[0]] for i in y_p]\n",
    "y_final = np.array(y_final)\n",
    "df_pred = pd.DataFrame(data={'Longitude':y_final[:,0], 'Latitude':y_final[:,1]})\n",
    "df_pred.to_csv('pred5.csv', index=False)\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
